{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as tr \n",
    "\n",
    "from dataset import make_dataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n",
      "(tensor([4]), tensor([4]), tensor([4]), tensor([4]))\n",
      "(tensor([[ 200.9026,    0.0000, 1158.6770,  960.9201]]), tensor([[ 266.9031,  348.4540, 1536.1183,  878.4917]]), tensor([[ 897.0312,    0.0000, 1280.0000,  720.0000]]), tensor([[ 348.3173,  163.5216, 1481.7118,  965.0387]]))\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from data.dataset import make_dataset, collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 설정 변수\n",
    "batch_size = 4\n",
    "size = 512\n",
    "\n",
    "image_dir_train = 'train/images/'\n",
    "bbox_dir_train = 'train/bbox/'\n",
    "\n",
    "image_dir_val = 'validation/images/'\n",
    "bbox_dir_val = 'validation/bbox/'\n",
    "\n",
    "image_dir_test = 'test/images/'\n",
    "bbox_dir_test = 'test/bbox/'\n",
    "\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_set = make_dataset(\n",
    "    image_dir=image_dir_train,\n",
    "    bbox_dir=bbox_dir_train\n",
    ")\n",
    "\n",
    "val_set = make_dataset(\n",
    "    image_dir=image_dir_val,\n",
    "    bbox_dir=bbox_dir_val\n",
    ")\n",
    "\n",
    "test_set = make_dataset(\n",
    "    image_dir=image_dir_test,\n",
    "    bbox_dir=bbox_dir_test\n",
    ")\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(\n",
    "    train_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# 데이터 로드 및 출력\n",
    "images, labels, bbox = next(iter(val_loader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels)\n",
    "print(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt to 'DL_Programming\\yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\y'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\y'\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_2644\\3928075647.py:1: SyntaxWarning: invalid escape sequence '\\y'\n",
      "  model = YOLO('DL_Programming\\yolov8n.pt')\n",
      "100%|██████████| 6.23M/6.23M [00:00<00:00, 18.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('DL_Programming\\yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 80\n",
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "print(type(model.names), len(model.names))\n",
    "print(model.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.28  Python-3.12.2 torch-2.2.2 CPU (11th Gen Intel Core(TM) i5-1155G7 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=DL_Programming\\yolov8n.pt, data=C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\data.yaml, epochs=10, time=None, patience=2, batch=32, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train3\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to 'C:\\Users\\User\\AppData\\Roaming\\Ultralytics\\Arial.ttf'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 755k/755k [00:00<00:00, 38.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752287  ultralytics.nn.modules.head.Detect           [5, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011823 parameters, 3011807 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\User\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\User\\OneDrive\\바탕 화면\\DL\\DL_Programming\\data\\wandb\\run-20240607_195038-whh9n5nc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/interactive_ai/YOLOv8/runs/whh9n5nc' target=\"_blank\">train3</a></strong> to <a href='https://wandb.ai/interactive_ai/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/interactive_ai/YOLOv8' target=\"_blank\">https://wandb.ai/interactive_ai/YOLOv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/interactive_ai/YOLOv8/runs/whh9n5nc' target=\"_blank\">https://wandb.ai/interactive_ai/YOLOv8/runs/whh9n5nc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\User\\OneDrive\\바탕 화면\\DL\\DL_Programming\\data\\train\\labels... 8421 images, 0 backgrounds, 32 corrupt: 100%|██████████| 8421/8421 [00:06<00:00, 1320.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_009_00046.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0048      1.0029]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_010_00326.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011      1.0029]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_010_00359.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0021]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_042_01171.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_042_07627.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_042_08641.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0023]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_045_00639.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_045_03356.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_045_03487.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_045_03530.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_046_04100.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011      1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\bundle of ropes_048_03584.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0023]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\circular fish trap_004_02328.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_004_00817.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_004_02354.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_004_03018.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_004_03545.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0005      1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_004_04351.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\fish net_029_02342.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.2064       1.279]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\rope_045_01626.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\rope_045_01663.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\rope_045_02420.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\spring fish trap_029_01799.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0011]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\spring fish trap_029_01907.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\spring fish trap_044_06201.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0007]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\tire_004_00418.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\tire_045_00547.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\tire_045_02135.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0062]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\tire_045_03539.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\tire_045_04412.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0062]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\wood_047_03039.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\images\\wood_048_04596.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0023]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\train\\labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\User\\OneDrive\\바탕 화면\\DL\\DL_Programming\\data\\validation\\labels... 2406 images, 0 backgrounds, 4 corrupt: 100%|██████████| 2406/2406 [00:01<00:00, 2124.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\validation\\images\\bundle of ropes_045_02766.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0018]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\validation\\images\\fish net_004_01515.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0004]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\validation\\images\\spring fish trap_045_04682.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0002]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING  C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\validation\\images\\tire_042_09867.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [      1.004]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\User\\OneDrive\\ \\DL\\DL_Programming\\data\\validation\\labels.cache\n",
      "Plotting labels to runs\\detect\\train3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.187      3.937      1.564         32        512:   2%|▏         | 5/263 [02:23<2:03:33, 28.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUser\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m바탕 화면\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDL\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDL_Programming\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\ultralytics\\engine\\model.py:674\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:199\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:379\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    375\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    376\u001b[0m     )\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\deeplearning\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(data=r'C:\\Users\\User\\OneDrive\\바탕 화면\\DL\\DL_Programming\\data\\data.yaml', epochs=10, patience=2, batch=32, imgsz=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model.names), len(model.names))\n",
    "print(model.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(source='data\\test\\images', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from typing import Type\n",
    "import torch\n",
    "import torchvision.transforms as tr\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, bbox_dir, transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.bbox_dir = bbox_dir\n",
    "        self.data_images = [file for file in os.listdir(image_dir) if file.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "        # 레이블 매핑 딕셔너리\n",
    "        self.label_map = {\n",
    "            'tire': 0,\n",
    "            'spring fish trap': 1,\n",
    "            'circular fish trap': 1,\n",
    "            'rectangular fish trap': 1,\n",
    "            'eel fish trap': 1,\n",
    "            'fish net': 2,\n",
    "            'wood': 3,\n",
    "            'rope': 4,\n",
    "            'bundle of ropes': 4\n",
    "        }\n",
    "\n",
    "    def parse_bbox_xml(self, xml_file, image_width, image_height):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            label = self.label_map.get(name, -1)  # 레이블 매핑, 없으면 -1\n",
    "            \n",
    "            if label == -1:\n",
    "                continue\n",
    "\n",
    "            bbox = obj.find('bndbox')\n",
    "            xmin = float(bbox.find('xmin').text) / image_width\n",
    "            ymin = float(bbox.find('ymin').text) / image_height\n",
    "            xmax = float(bbox.find('xmax').text) / image_width\n",
    "            ymax = float(bbox.find('ymax').text) / image_height\n",
    "\n",
    "            # 바운딩 박스의 중심점과 너비, 높이 계산\n",
    "            x_center = (xmin + xmax) / 2\n",
    "            y_center = (ymin + ymax) / 2\n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "\n",
    "            boxes.append([x_center, y_center, width, height])\n",
    "            labels.append(label)\n",
    "\n",
    "        # 객체가 1개만 있는 경우만 반환\n",
    "        if len(boxes) == 1 and len(labels) == 1:\n",
    "            return boxes, labels\n",
    "        else:\n",
    "            return None, None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.data_images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_file)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image_width, image_height = image.size\n",
    "\n",
    "        # xml 파일 경로 찾기\n",
    "        xml_file = os.path.join(self.bbox_dir, img_file.replace('.jpg', '.xml'))\n",
    "\n",
    "        boxes, labels = self.parse_bbox_xml(xml_file, image_width, image_height)\n",
    "        \n",
    "        # boxes 또는 labels가 None이면 다음 데이터로 넘어감\n",
    "        if boxes is None or labels is None:\n",
    "            return self.__getitem__((idx + 1) % len(self.data_images))\n",
    "\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return image, labels, boxes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "\n",
    "def make_dataset(image_dir: str,\n",
    "                 bbox_dir: str,\n",
    "                 transform=tr.Compose([tr.Resize((512, 512)), \n",
    "                                       tr.ToTensor(),                                       \n",
    "                                       tr.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                       ])\n",
    "                 ) -> Type[torch.utils.data.Dataset]:\n",
    "    \"\"\"\n",
    "    Make pytorch Dataset for given task.\n",
    "    Read the image using the PIL library and return it as an np.array.\n",
    "\n",
    "    Args:\n",
    "        image_dir (str): dataset directory\n",
    "        bbox_dir (str) : dataset directory\n",
    "        transform (torchvision.transforms) pytorch image transforms  \n",
    "\n",
    "    Returns:\n",
    "        torch.Dataset: pytorch Dataset\n",
    "    \"\"\"\n",
    "        \n",
    "    dataset = CustomDataset(image_dir=image_dir,\n",
    "                            bbox_dir=bbox_dir,\n",
    "                            transform=transform)\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels, boxes = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, labels, boxes\n",
    "\n",
    "# 저장할 경로\n",
    "image_dir_train = 'train/images/'\n",
    "bbox_dir_train = 'train/bbox/'\n",
    "\n",
    "image_dir_val = 'validation/images/'\n",
    "bbox_dir_val = 'validation/bbox/'\n",
    "\n",
    "image_dir_test = 'test/images/'\n",
    "bbox_dir_test = 'test/bbox/'\n",
    "\n",
    "# 라벨 파일 생성 및 저장\n",
    "def save_label_file(image_file, labels, boxes, output_dir):\n",
    "    label_file = image_file.replace('.jpg', '.txt')\n",
    "    with open(os.path.join(output_dir, label_file), 'w') as f:\n",
    "        for label, box in zip(labels, boxes):\n",
    "            x_center, y_center, width, height = box\n",
    "            line = f\"{label} {x_center} {y_center} {width} {height}\\n\"\n",
    "            f.write(line)\n",
    "\n",
    "def save_label_files(dataset, image_dir, bbox_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    for idx in range(len(dataset)):\n",
    "        image, labels, boxes = dataset[idx]\n",
    "        image_file = dataset.data_images[idx]\n",
    "        save_label_file(image_file, labels, boxes, output_dir)\n",
    "\n",
    "# train set\n",
    "train_dataset = make_dataset(image_dir_train, bbox_dir_train)\n",
    "save_label_files(train_dataset, image_dir_train, bbox_dir_train, 'train/labels')\n",
    "\n",
    "# validation set\n",
    "val_dataset = make_dataset(image_dir_val, bbox_dir_val)\n",
    "save_label_files(val_dataset, image_dir_val, bbox_dir_val, 'validation/labels')\n",
    "\n",
    "# test set\n",
    "# test_dataset = make_dataset(image_dir_test, bbox_dir_test)\n",
    "# save_label_files(test_dataset, image_dir_test, bbox_dir_test, 'test/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_yolo_label(label_file):\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    labels = []\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        class_id, x_center, y_center, width, height = map(float, parts)\n",
    "        labels.append((int(class_id), x_center, y_center, width, height))\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def draw_bounding_boxes(image, labels):\n",
    "    h, w, _ = image.shape\n",
    "    for class_id, x_center, y_center, width, height in labels:\n",
    "        x_min = int((x_center - width/2) * w)\n",
    "        y_min = int((y_center - height/2) * h)\n",
    "        x_max = int((x_center + width/2) * w)\n",
    "        y_max = int((y_center + height/2) * h)\n",
    "        \n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f'Class: {class_id}', (x_min, y_min - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# 이미지 및 레이블 파일 경로\n",
    "image_path = 'test/images/wood_048_04863.jpg'\n",
    "label_file = 'test/labels/wood_048_04863.txt'\n",
    "\n",
    "# 이미지 읽기\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"이미지를 불러올 수 없습니다: {image_path}\")\n",
    "    exit()\n",
    "\n",
    "# 레이블 읽기\n",
    "labels = read_yolo_label(label_file)\n",
    "\n",
    "# Bounding box 그리기\n",
    "image_with_boxes = draw_bounding_boxes(image.copy(), labels)\n",
    "\n",
    "# 결과 이미지 표시\n",
    "cv2.imshow('Image with Bounding Boxes', image_with_boxes)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
